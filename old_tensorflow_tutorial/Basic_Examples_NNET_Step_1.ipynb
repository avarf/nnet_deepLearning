{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "dataset_address = \"./mnist/\"\n",
    "mnist = input_data.read_data_sets(dataset_address, one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print ((mnist.train.images[0,:]).size)\n",
    "print (mnist.train.images.shape)\n",
    "print (mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design The Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Constants\n",
    "\n",
    "Number_Classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.12\n",
    "Image_Pixels = (mnist.train.images[0,:]).size\n",
    "hidden_1_neurons = 15\n",
    "hidden_2_neurons = 8\n",
    "hidden_3_neurons = 0\n",
    "outLayer_neurons = Number_Classes\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(None, Image_Pixels))\n",
    "labels_placeholder = tf.placeholder(tf.float32, shape=(batch_size))\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "\n",
    "# Hidden 1\n",
    "with tf.name_scope('hidden_1'):\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([Image_Pixels, hidden_1_neurons],\n",
    "                                               stddev=1.0/math.sqrt(float(Image_Pixels))), name='weights')\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_1_neurons]), name='biases')\n",
    "    hidden_1_out = tf.nn.relu(tf.matmul(images_placeholder, weights_1) + biases_1)\n",
    "# Hidden 2\n",
    "with tf.name_scope('hidden_2'):\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_1_neurons, hidden_2_neurons],\n",
    "                                               stddev=1.0/math.sqrt(float(hidden_1_neurons))), name='weights')\n",
    "    biases_2 = tf.Variable(tf.zeros([hidden_2_neurons]), name='biases')\n",
    "    hidden_2_out = tf.nn.relu(tf.matmul(hidden_1_out, weights_2) + biases_2)\n",
    "# Output Layer\n",
    "with tf.name_scope('out'):\n",
    "    weights_out = tf.Variable(tf.truncated_normal([hidden_2_neurons, Number_Classes],\n",
    "                                               stddev=1.0/math.sqrt(float(hidden_2_neurons))), name='weights')\n",
    "    biases_out = tf.Variable(tf.zeros([Number_Classes]), name='biases')\n",
    "    outLayer_output = tf.matmul(hidden_2_out, weights_out) + biases_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The NNET\n",
    "\n",
    "- Before Variables can be used within a session, they must be initialized using that session. This step takes the initial values (in this case tensors full of zeros) that have already been specified, and assigns them to each Variable. This can be done for all Variables at once:\n",
    "    - sess.run(tf.global_variables_initializer())\n",
    "- We can specify a loss function just as easily. Loss indicates how bad the model's prediction was on a single example; we try to minimize that while training across all the examples. Here, our loss function is the cross-entropy between the target and the softmax activation function applied to the model's prediction. As in the beginners tutorial, we use the stable formulation:\n",
    "    - One very common, very nice function to determine the loss of a model is called \"cross-entropy.\"\n",
    "    - To implement cross-entropy we need to first add a new placeholder to input the correct answers: expected_output = tf.placeholder(tf.float32, shape=[None, Number_Classes]) \n",
    "    - cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=expected_output, logits=outLayer_output))\n",
    "    - Note that tf.nn.softmax_cross_entropy_with_logits internally applies the softmax on the model's unnormalized model prediction and sums across all classes, and tf.reduce_mean takes the average over these sums.\n",
    "- Because TensorFlow knows the entire graph of your computations, it can automatically use the backpropagation algorithm to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss:\n",
    "    - train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "- Training iteration\n",
    "- We load 100 training examples in each training iteration. We then run the train_step operation, using feed_dict to replace the placeholder tensors x and y_ with the training examples. Note that you can replace any tensor in your computation graph using feed_dict -- it's not restricted to just placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9303\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "expected_output = tf.placeholder(tf.float32, shape=[None, Number_Classes])\n",
    "\n",
    "# loss function\n",
    "cross_entropy = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=expected_output, logits=outLayer_output))\n",
    "\n",
    "# train steps and method\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# training iteration\n",
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={images_placeholder: batch[0], expected_output: batch[1]})\n",
    "\n",
    "# evaluation and testing\n",
    "correct_prediction = tf.equal(tf.argmax(outLayer_output, 1), tf.argmax(expected_output, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={images_placeholder: mnist.test.images, expected_output: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
